{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5B5qz6Bae9G",
        "outputId": "dce56e4a-a018-4906-b352-244f5fb15b99"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 31.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-22.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.24.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqGzOteanAq",
        "outputId": "ff9f3e81-36f6-40fd-9b0a-fc7cf8c46ad3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.8.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n",
            "Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.19.6)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.1.97)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers[sentencepiece]) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForSequenceClassification,\n",
        ")\n",
        "from typing import Any, List, Mapping, Tuple\n",
        "\n",
        "\n",
        "class QuestionGenerator:\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        QG_PRETRAINED = \"iarfmoose/t5-base-question-generator\"\n",
        "        self.ANSWER_TOKEN = \"<answer>\"\n",
        "        self.CONTEXT_TOKEN = \"<context>\"\n",
        "        self.SEQ_LENGTH = 512\n",
        "\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.qg_tokenizer = AutoTokenizer.from_pretrained(\n",
        "            QG_PRETRAINED, use_fast=False)\n",
        "        self.qg_model = AutoModelForSeq2SeqLM.from_pretrained(QG_PRETRAINED)\n",
        "        self.qg_model.to(self.device)\n",
        "        self.qg_model.eval()\n",
        "\n",
        "        self.qa_evaluator = QAEvaluator()\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        article: str,\n",
        "        num_questions: bool = None,\n",
        "    ) -> List:\n",
        "        print(\"Loading questions:\\n\")\n",
        "        qg_inputs, qg_answers = self.generate_qg_inputs(article)\n",
        "        generated_questions = self.generate_questions_from_inputs(qg_inputs)\n",
        "        encoded_qa_pairs = self.qa_evaluator.encode_qa_pairs(\n",
        "            generated_questions, qg_answers\n",
        "        )\n",
        "        scores = self.qa_evaluator.get_scores(encoded_qa_pairs)\n",
        "        if num_questions:\n",
        "            qa_list = self._get_ranked_qa_pairs(\n",
        "                generated_questions, qg_answers, scores, num_questions\n",
        "            )\n",
        "        else:\n",
        "            qa_list = self._get_ranked_qa_pairs(\n",
        "                generated_questions, qg_answers, scores\n",
        "            )\n",
        "        return qa_list\n",
        "\n",
        "    def generate_qg_inputs(self, text: str) -> Tuple[List[str], List[str]]:\n",
        "        inputs = []\n",
        "        answers = []\n",
        "\n",
        "        sentences = self._split_text(text)\n",
        "        prepped_inputs, prepped_answers = self._prepare_qg_inputs_MC(\n",
        "            sentences\n",
        "        )\n",
        "        inputs.extend(prepped_inputs)\n",
        "        answers.extend(prepped_answers)\n",
        "        return inputs, answers\n",
        "\n",
        "    def generate_questions_from_inputs(self, qg_inputs: List) -> List[str]:\n",
        "        generated_questions = []\n",
        "\n",
        "        for qg_input in qg_inputs:\n",
        "            question = self._generate_question(qg_input)\n",
        "            generated_questions.append(question)\n",
        "\n",
        "        return generated_questions\n",
        "\n",
        "    def _split_text(self, text: str) -> List[str]:\n",
        "        MAX_SENTENCE_LEN = 128\n",
        "        sentences = re.findall(\".*?[.!\\?]\", text)\n",
        "        cut_sentences = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(sentence) > MAX_SENTENCE_LEN:\n",
        "                cut_sentences.extend(re.split(\"[,;:)]\", sentence))\n",
        "        cut_sentences = [s for s in sentences if len(s.split(\" \")) > 5]\n",
        "        sentences = sentences + cut_sentences\n",
        "\n",
        "        return list(set([s.strip(\" \") for s in sentences]))\n",
        "\n",
        "    def _split_into_segments(self, text: str) -> List[str]:\n",
        "\n",
        "        MAX_TOKENS = 490\n",
        "        paragraphs = text.split(\"\\n\")\n",
        "        tokenized_paragraphs = [\n",
        "            self.qg_tokenizer(p)[\"input_ids\"] for p in paragraphs if len(p) > 0\n",
        "        ]\n",
        "        segments = []\n",
        "\n",
        "        while len(tokenized_paragraphs) > 0:\n",
        "            segment = []\n",
        "            while len(segment) < MAX_TOKENS and len(tokenized_paragraphs) > 0:\n",
        "                paragraph = tokenized_paragraphs.pop(0)\n",
        "                segment.extend(paragraph)\n",
        "            segments.append(segment)\n",
        "\n",
        "        return [self.qg_tokenizer.decode(s, skip_special_tokens=True) for s in segments]\n",
        "\n",
        "    def _prepare_qg_inputs_MC(self, sentences: List[str]) -> Tuple[List[str], List[str]]:\n",
        "\n",
        "        spacy_nlp = en_core_web_sm.load()\n",
        "        docs = list(spacy_nlp.pipe(sentences, disable=[\"parser\"]))\n",
        "        inputs_from_text = []\n",
        "        answers_from_text = []\n",
        "        for doc, sentence in zip(docs, sentences):\n",
        "            entities = doc.ents\n",
        "            if entities:\n",
        "\n",
        "                for entity in entities:\n",
        "                    qg_input = f\"{self.ANSWER_TOKEN} {entity} {self.CONTEXT_TOKEN} {sentence}\"\n",
        "                    answers = self._get_MC_answers(entity, docs)\n",
        "                    inputs_from_text.append(qg_input)\n",
        "                    answers_from_text.append(answers)\n",
        "\n",
        "        return inputs_from_text, answers_from_text\n",
        "\n",
        "    def _get_MC_answers(self, correct_answer: Any, docs: Any) -> List[Mapping[str, Any]]:\n",
        "        entities = []\n",
        "        for doc in docs:\n",
        "            entities.extend([{\"text\": e.text, \"label_\": e.label_}\n",
        "                            for e in doc.ents])\n",
        "        entities_json = [json.dumps(kv) for kv in entities]\n",
        "        pool = set(entities_json)\n",
        "        num_choices = (\n",
        "            min(4, len(pool)) - 1\n",
        "        ) \n",
        "        final_choices = []\n",
        "        correct_label = correct_answer.label_\n",
        "        final_choices.append({\"answer\": correct_answer.text, \"correct\": True})\n",
        "        pool.remove(\n",
        "            json.dumps({\"text\": correct_answer.text,\n",
        "                       \"label_\": correct_answer.label_})\n",
        "        )\n",
        "        matches = [e for e in pool if correct_label in e]\n",
        "        if len(matches) < num_choices:\n",
        "            choices = matches\n",
        "            pool = pool.difference(set(choices))\n",
        "            choices.extend(random.sample(pool, num_choices - len(choices)))\n",
        "        else:\n",
        "            choices = random.sample(matches, num_choices)\n",
        "\n",
        "        choices = [json.loads(s) for s in choices]\n",
        "\n",
        "        for choice in choices:\n",
        "            final_choices.append({\"answer\": choice[\"text\"], \"correct\": False})\n",
        "\n",
        "        random.shuffle(final_choices)\n",
        "        return final_choices\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _generate_question(self, qg_input: str) -> str:\n",
        "        encoded_input = self._encode_qg_input(qg_input)\n",
        "        output = self.qg_model.generate(input_ids=encoded_input[\"input_ids\"])\n",
        "        question = self.qg_tokenizer.decode(\n",
        "            output[0],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "        return question\n",
        "\n",
        "    def _encode_qg_input(self, qg_input: str) -> torch.tensor:\n",
        "        return self.qg_tokenizer(\n",
        "            qg_input,\n",
        "            padding='max_length',\n",
        "            max_length=self.SEQ_LENGTH,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(self.device)\n",
        "\n",
        "    def _get_ranked_qa_pairs(\n",
        "        self, generated_questions: List[str], qg_answers: List[str], scores, num_questions: int = 10\n",
        "    ) -> List[Mapping[str, str]]:\n",
        "        qa_list = []\n",
        "        for i in range(num_questions):\n",
        "            index = scores[i]\n",
        "            qa = {\n",
        "                \"question\": generated_questions[index].split(\"?\")[0] + \"?\",\n",
        "                \"answer\": qg_answers[index]\n",
        "            }\n",
        "            qa_list.append(qa)\n",
        "\n",
        "        return qa_list\n",
        "\n",
        "    def _get_all_qa_pairs(self, generated_questions: List[str], qg_answers: List[str]):\n",
        "        qa_list = []\n",
        "\n",
        "        for question, answer in zip(generated_questions, qg_answers):\n",
        "            qa = {\n",
        "                \"Q\": question.split(\"?\")[0] + \"?\",\n",
        "                \"A\": answer\n",
        "            }\n",
        "            qa_list.append(qa)\n",
        "        return qa_list\n",
        "\n",
        "\n",
        "class QAEvaluator:\n",
        "    def __init__(self) -> None:\n",
        "        QAE_PRETRAINED = \"iarfmoose/bert-base-cased-qa-evaluator\"\n",
        "        self.SEQ_LENGTH = 512\n",
        "\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.qae_tokenizer = AutoTokenizer.from_pretrained(QAE_PRETRAINED)\n",
        "        self.qae_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            QAE_PRETRAINED\n",
        "        )\n",
        "        self.qae_model.to(self.device)\n",
        "        self.qae_model.eval()\n",
        "\n",
        "    def encode_qa_pairs(self, questions: List[str], answers: List[str]) -> List[torch.tensor]:\n",
        "        encoded_pairs = []\n",
        "\n",
        "        for question, answer in zip(questions, answers):\n",
        "            encoded_qa = self._encode_qa(question, answer)\n",
        "            encoded_pairs.append(encoded_qa.to(self.device))\n",
        "        return encoded_pairs\n",
        "\n",
        "    def get_scores(self, encoded_qa_pairs: List[torch.tensor]) -> List[float]:\n",
        "        scores = {}\n",
        "        for i in range(len(encoded_qa_pairs)):\n",
        "            scores[i] = self._evaluate_qa(encoded_qa_pairs[i])\n",
        "\n",
        "        return [\n",
        "            k for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
        "        ]\n",
        "\n",
        "    def _encode_qa(self, question: str, answer: str) -> torch.tensor:\n",
        "        if type(answer) is list:\n",
        "            for a in answer:\n",
        "                if a[\"correct\"]:\n",
        "                    correct_answer = a[\"answer\"]\n",
        "        else:\n",
        "            correct_answer = answer\n",
        "\n",
        "        return self.qae_tokenizer(\n",
        "            text=question,\n",
        "            text_pair=correct_answer,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.SEQ_LENGTH,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _evaluate_qa(self, encoded_qa_pair: torch.tensor) -> float:\n",
        "        output = self.qae_model(**encoded_qa_pair)\n",
        "        return output[0][0][1]\n",
        "\n",
        "\n",
        "def print_qa(qa_list: List[Mapping[str, str]], show_answers: bool = True) -> None:\n",
        "    score = 0\n",
        "    for i in range(len(qa_list)):\n",
        "        space = \" \" * int(np.where(i < 9, 3, 4))\n",
        "        print(f\"{i + 1}) Q: {qa_list[i]['question']}\")\n",
        "        answer = qa_list[i][\"answer\"]\n",
        "        if type(answer) is list:\n",
        "\n",
        "            if show_answers:\n",
        "                print(\n",
        "                    f\"{space}A: 1. {answer[0]['answer']} \"\n",
        "                    f\"{np.where(answer[0]['correct'], '(correct)', '')}\"\n",
        "                )\n",
        "                for j in range(1, len(answer)):\n",
        "                    print(\n",
        "                        f\"{space + '   '}{j + 1}. {answer[j]['answer']} \"\n",
        "                        f\"{np.where(answer[j]['correct']==True,'(correct)', '')}\"\n",
        "                    )\n",
        "\n",
        "            else:\n",
        "                \n",
        "                answerRight = -1\n",
        "                print(f\"{space}A: 1. {answer[0]['answer']}\")\n",
        "                for j in range(1, len(answer)):\n",
        "                    print(f\"{space + '   '}{j + 1}. {answer[j]['answer']}\")\n",
        "                    if(answer[j]['correct']==True):\n",
        "                      answerRight = j+1\n",
        "                print(\"Enter the correct option:\")\n",
        "                s=int(input(\" \"))\n",
        "                if(s==answerRight):\n",
        "                  print(\"Correct answer\")\n",
        "                  score = score+1\n",
        "                else:\n",
        "                  print(\"Wrong answer\")\n",
        "            print(\"\")\n",
        "        else:\n",
        "            if show_answers:\n",
        "                print(f\"{space}A: {answer}\\n\")\n",
        "    print(\"Your total score is \",score)"
      ],
      "metadata": {
        "id": "j0iWqTwxHhRi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "assert device == torch.device('cuda'), \"Not using CUDA. Set: Runtime > Change runtime type > Hardware Accelerator: GPU\""
      ],
      "metadata": {
        "id": "sYnOzxmxAOa_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF3xjRCqZ4s6",
        "outputId": "d8b0bc2d-a4f9-4991-a4bd-f7dcf38cb7b8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ques_gen = QuestionGenerator()"
      ],
      "metadata": {
        "id": "XA_eOhzBPXRy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('original_text.txt','r') as t:\n",
        "  original_text=t.read()"
      ],
      "metadata": {
        "id": "fMC6RfVHAxGW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = ques_gen.generate(\n",
        "    original_text,\n",
        "    num_questions=10, \n",
        ")\n",
        "print_qa(questions,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0YqDWM6CmR2",
        "outputId": "0b500fff-9850-450c-ee33-e6c2367a8288"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading questions:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Q: where did you receive your MD?\n",
            "   A: 1. Tulane University School of Medicine\n",
            "      2. Meat Inspection\n",
            "      3. Focus\n",
            "      4. Cover\n",
            "Enter the correct option:\n",
            " 1\n",
            "Wrong answer\n",
            "\n",
            "2) Q: Who is now allowed to use a one in million standard for carcinogens?\n",
            "   A: 1. Bureau\n",
            "      2. the Food and Drug Administration\n",
            "      3. Time Magazine\n",
            "      4. Target\n",
            "Enter the correct option:\n",
            " 1\n",
            "Wrong answer\n",
            "\n",
            "3) Q: how long before Thanksgiving?\n",
            "   A: 1. the early 1900s\n",
            "      2. 1911\n",
            "      3. 1900\n",
            "      4. 17 days\n",
            "Enter the correct option:\n",
            " 1\n",
            "Wrong answer\n",
            "\n",
            "4) Q: when did muckraking come into being?\n",
            "   A: 1. 100 years old\n",
            "      2. more than a quarter-century\n",
            "      3. the early 1900s\n",
            "      4. the 1960s\n",
            "Enter the correct option:\n",
            " 1\n",
            "Wrong answer\n",
            "\n",
            "5) Q: How long has a doctor been practicing?\n",
            "   A: 1. 4 years\n",
            "      2. years\n",
            "      3. more than a quarter-century\n",
            "      4. the early 1900s\n",
            "Enter the correct option:\n",
            " 1\n",
            "Wrong answer\n",
            "\n",
            "6) Q: how old is meat inspection?\n",
            "   A: 1. 17 days\n",
            "      2. 100 years old\n",
            "      3. the past 15 years\n",
            "      4. the early 1900s\n",
            "Enter the correct option:\n",
            " 1\n",
            "Wrong answer\n",
            "\n",
            "7) Q: what was the delaney clause?\n",
            "   A: 1. Bureau\n",
            "      2. Food Lion\n",
            "      3. the Food and Drug Administration\n",
            "      4. the Delaney Clause\n",
            "Enter the correct option:\n",
            " 1\n",
            "Wrong answer\n",
            "\n",
            "8) Q: When did the United States have several incidents of industrialization and urbanization in the late 1960s?\n",
            "   A: 1. 1958\n",
            "      2. the early 1900s\n",
            "      3. more than a quarter-century\n",
            "      4. early 70s\n",
            "Enter the correct option:\n",
            " 1\n",
            "Wrong answer\n",
            "\n",
            "9) Q: how long have you been in the US?\n",
            "   A: 1. these past few days\n",
            "      2. 1906\n",
            "      3. 17 days\n",
            "      4. the past 15 years\n",
            "Enter the correct option:\n",
            " 1\n",
            "Wrong answer\n",
            "\n",
            "10) Q: how many people are involved in agriculture?\n",
            "    A: 1. thirty 40%\n",
            "       2. ADI\n",
            "       3. MD\n",
            "       4. two to four percent\n",
            "Enter the correct option:\n",
            " 1\n",
            "Wrong answer\n",
            "\n",
            "Your total score is  0\n"
          ]
        }
      ]
    }
  ]
}